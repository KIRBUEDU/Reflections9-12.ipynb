{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec85561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework Reflection 9 Question 1\n",
    "# Write some code that will use a simulation to estimate the standard deviation of the coefficient when there is heteroskedasticity.  \n",
    "# Compare these standard errors to those found via statsmodels OLS or a similar linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff86e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical SD of beta_X      : 0.0877\n",
      "Avg OLS SE (assumes homosk.) : 0.0600  (ratio=0.684, under by 31.6%)\n",
      "Avg Robust SE (HC1)         : 0.0843  (ratio=0.961, under by 3.9%)\n",
      "Avg Robust SE (HC3)         : 0.0847  (ratio=0.966, under by 3.4%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 1000        \n",
    "reps = 500      \n",
    "DO_COVERAGE = True  \n",
    "\n",
    "\n",
    "betas = []\n",
    "se_ols = []\n",
    "se_hc1 = []\n",
    "se_hc3 = []\n",
    "\n",
    "\n",
    "for _ in range(reps):\n",
    "   \n",
    "    X = np.random.normal(0, 1, n)\n",
    "    Z = np.random.normal(0, 1, n)  \n",
    "\n",
    "    \n",
    "    eps = np.random.normal(0, 1 + np.abs(X), n)\n",
    "\n",
    "   \n",
    "    Y = 2*X + 1*Z + eps\n",
    "\n",
    "    \n",
    "    Xmat = sm.add_constant(np.column_stack([X, Z]))\n",
    "    m = sm.OLS(Y, Xmat).fit()\n",
    "\n",
    "   \n",
    "    betas.append(m.params[1])                     \n",
    "    se_ols.append(m.bse[1])                       \n",
    "    se_hc1.append(m.get_robustcov_results(cov_type='HC1').bse[1])  \n",
    "    se_hc3.append(m.get_robustcov_results(cov_type='HC3').bse[1])  \n",
    "\n",
    "\n",
    "betas = np.array(betas)\n",
    "se_ols = np.array(se_ols)\n",
    "se_hc1 = np.array(se_hc1)\n",
    "se_hc3 = np.array(se_hc3)\n",
    "\n",
    "\n",
    "emp_sd = betas.std(ddof=1)\n",
    "avg_ols = se_ols.mean()\n",
    "avg_hc1 = se_hc1.mean()\n",
    "avg_hc3 = se_hc3.mean()\n",
    "\n",
    "ratio_ols   = avg_ols / emp_sd\n",
    "ratio_hc1   = avg_hc1 / emp_sd\n",
    "ratio_hc3   = avg_hc3 / emp_sd\n",
    "under_ols   = 100 * (1 - ratio_ols)\n",
    "under_hc1   = 100 * (1 - ratio_hc1)\n",
    "under_hc3   = 100 * (1 - ratio_hc3)\n",
    "\n",
    "\n",
    "print(f\"Empirical SD of beta_X      : {emp_sd:.4f}\")\n",
    "print(f\"Avg OLS SE (assumes homosk.) : {avg_ols:.4f}  \"\n",
    "      f\"(ratio={ratio_ols:.3f}, under by {under_ols:.1f}%)\")\n",
    "print(f\"Avg Robust SE (HC1)         : {avg_hc1:.4f}  \"\n",
    "      f\"(ratio={ratio_hc1:.3f}, under by {under_hc1:.1f}%)\")\n",
    "print(f\"Avg Robust SE (HC3)         : {avg_hc3:.4f}  \"\n",
    "      f\"(ratio={ratio_hc3:.3f}, under by {under_hc3:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1662e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets just say we are trying to guess how heavy a bag is and we it 500 times, the guesses jump around \n",
    "# Emperical SD means how much the estimate varies when we repeat the measurement every time \n",
    "# Emperical SD - the average amoount of the variation \n",
    "# OLS SE = 0.06 it is off by 32% - being overconfindent - making us think that we are more precise to the actual weight than in reality what it is \n",
    "# Robust SE - almost matches the prediction it is just off by 3 - 4%\n",
    "# Robust SE - gives a more honest answer when the data is uneven "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfde27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework Reflection 9 Question 2\n",
    "# Write some code that will use a simulation to estimate the standard deviation of the coefficient when errors are highly correlated / non-independent.\n",
    "# Compare these standard errors to those found via statsmodels OlS or a similar linear regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90f9f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical SD : 0.07012066915541845\n",
      "Avg OLS SE   : 0.07156014914836889\n",
      "Avg HAC SE   : 0.07122063189469006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(0)\n",
    "n, reps, rho = 1000, 500, 0.9\n",
    "\n",
    "betas, se_ols, se_hac = [], [], []\n",
    "\n",
    "for _ in range(reps):\n",
    "    X = np.random.normal(0, 1, n)\n",
    "\n",
    "    \n",
    "    e = np.zeros(n)\n",
    "    for t in range(1, n):\n",
    "        e[t] = rho * e[t-1] + np.random.normal()\n",
    "\n",
    "    Y = 2*X + e\n",
    "\n",
    "    m = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "    betas.append(m.params[1])\n",
    "    se_ols.append(m.bse[1])\n",
    "    se_hac.append(m.get_robustcov_results(cov_type='HAC', maxlags=1).bse[1])\n",
    "\n",
    "betas, se_ols, se_hac = map(np.array, (betas, se_ols, se_hac))\n",
    "\n",
    "print(\"Empirical SD :\", betas.std(ddof=1))\n",
    "print(\"Avg OLS SE   :\", se_ols.mean())\n",
    "print(\"Avg HAC SE   :\", se_hac.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the experiment is repeated many times the slop changes - Emperical SD \n",
    "# OLS SE – the regular method – 0.0716\n",
    "# HAC SE – the autocorrelation-robust method – 0.0712\n",
    "# In this case the normal method and the robust method both did a good job in making the prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that if the correlation between coefficients is high enough, then the estimated standard deviation of the coefficient, using bootstrap errors, \n",
    "# might not match that found by a full simulation of the Data Generating Process.  (This can be fixed if you have a huge amount of data for the bootstrap simulation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb6dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full DGP SD of beta1     : 0.3087   (ground truth)\n",
      "Bootstrap SD of beta1    : 0.2903   (resample residuals, X fixed)\n",
      "Bootstrap / Full ratio   : 0.941  (want ≈ 1.00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "n = 500            \n",
    "reps = 500         \n",
    "boot_reps = 500    \n",
    "rho = 0.99         \n",
    "beta1, beta2 = 2.0, 3.0\n",
    "sigma = 1.0\n",
    "\n",
    "def gen_X(n, rho):\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    u  = np.random.normal(0, 1, n)\n",
    "    x2 = rho * x1 + np.sqrt(max(1 - rho**2, 1e-12)) * u\n",
    "    return x1, x2\n",
    "\n",
    "\n",
    "beta1_full = []\n",
    "for _ in range(reps):\n",
    "    x1, x2 = gen_X(n, rho)\n",
    "    y = beta1 * x1 + beta2 * x2 + np.random.normal(0, sigma, n)\n",
    "    X = sm.add_constant(np.column_stack([x1, x2]))\n",
    "    beta1_full.append(sm.OLS(y, X).fit().params[1])\n",
    "emp_sd = np.std(beta1_full, ddof=1)\n",
    "\n",
    "\n",
    "x1_fix, x2_fix = gen_X(n, rho)\n",
    "y_fix = beta1 * x1_fix + beta2 * x2_fix + np.random.normal(0, sigma, n)\n",
    "X_fix = sm.add_constant(np.column_stack([x1_fix, x2_fix]))\n",
    "m_fix = sm.OLS(y_fix, X_fix).fit()\n",
    "resid = m_fix.resid\n",
    "yhat  = m_fix.fittedvalues\n",
    "\n",
    "beta1_boot = []\n",
    "for _ in range(boot_reps):\n",
    "    y_boot = yhat + np.random.choice(resid, size=n, replace=True)\n",
    "    beta1_boot.append(sm.OLS(y_boot, X_fix).fit().params[1])\n",
    "boot_sd = np.std(beta1_boot, ddof=1)\n",
    "\n",
    "\n",
    "print(f\"Full DGP SD of beta1     : {emp_sd:.4f}   (ground truth)\")\n",
    "print(f\"Bootstrap SD of beta1    : {boot_sd:.4f}   (resample residuals, X fixed)\")\n",
    "print(f\"Bootstrap / Full ratio   : {boot_sd/emp_sd:.3f}  (want ≈ 1.00)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd993c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true amount the number moves around is 0.3087\n",
    "# The bootstrap method says it moves 0.2903\n",
    "# Bootstrap is a little too low because it didn’t let X change\n",
    "# With a lot more data, they would match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d09444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 11 Reflection Question 1 \n",
    "# Construct a dataset for an event study where the value, derivative, and second derivative of a trend all change discontinuously (suddenly) after an event.\n",
    "# Build a model that tries to decide whether the event is real (has a nonzero effect) using:\n",
    "# (a) only the value,\n",
    "# (b) the value, derivative, and second derivative.\n",
    "# Which of these models is better at detecting and/or quantifying the impact of the event?  (What might \"better\" mean here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76664b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One dataset:\n",
      "(a) value-only, p(D) = 1.491e-39\n",
      "(b) value+slope+curvature, joint p = 1.953e-42\n",
      "\n",
      "Detection rate (power, α=0.05) over 200 runs:\n",
      "(a) value-only               : 1.00\n",
      "(b) value+slope+curvature    : 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "T, t0 = 200, 100\n",
    "t = np.arange(T).astype(float)\n",
    "D = (t >= t0).astype(float)          \n",
    "tau = t - t0                        \n",
    "\n",
    "\n",
    "L, S, C = 1.0, 0.06, 0.001           \n",
    "y = 0.0 + 0.02*t - 0.0001*t**2 \\\n",
    "    + L*D + S*tau*D + C*(tau**2)*D \\\n",
    "    + np.random.normal(0, 0.3, T)\n",
    "\n",
    "\n",
    "m_a = sm.OLS(y, sm.add_constant(D)).fit()\n",
    "\n",
    "\n",
    "Xb = np.column_stack([\n",
    "    np.ones(T), t, t**2, D, D*tau, D*(tau**2)\n",
    "])\n",
    "m_b = sm.OLS(y, Xb).fit()\n",
    "\n",
    "print(\"One dataset:\")\n",
    "print(f\"(a) value-only, p(D) = {m_a.pvalues[1]:.4g}\")\n",
    "\n",
    "\n",
    "R = np.zeros((3, Xb.shape[1])); R[0,3]=R[1,4]=R[2,5]=1\n",
    "p_joint = float(m_b.f_test((R, np.zeros(3))).pvalue)\n",
    "print(f\"(b) value+slope+curvature, joint p = {p_joint:.4g}\")\n",
    "\n",
    "\n",
    "reps = 200\n",
    "hits_a = hits_b = 0\n",
    "for _ in range(reps):\n",
    "    eps = np.random.normal(0, 0.3, T)\n",
    "    y_sim = 0.0 + 0.02*t - 0.0001*t**2 + L*D + S*tau*D + C*(tau**2)*D + eps\n",
    "\n",
    "    pa = sm.OLS(y_sim, sm.add_constant(D)).fit().pvalues[1]\n",
    "    hits_a += (pa < 0.05)\n",
    "\n",
    "    mb = sm.OLS(y_sim, Xb).fit()\n",
    "    p_joint = float(mb.f_test((R, np.zeros(3))).pvalue)\n",
    "    hits_b += (p_joint < 0.05)\n",
    "\n",
    "print(\"\\nDetection rate (power, α=0.05) over\", reps, \"runs:\")\n",
    "print(f\"(a) value-only               : {hits_a/reps:.2f}\")\n",
    "print(f\"(b) value+slope+curvature    : {hits_b/reps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In all 200 tests, both models spotted the event every time \n",
    "# Model a - measures sudden jump \n",
    "# model b - measure the jump, change in speed, and the change in curvature \n",
    "# Model b - gives a more accurate and complete picture \n",
    "# Model b is better because it account for different factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea84e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 11 Reflection Question 2 \n",
    "# Construct a dataset in which there are three groups whose values each increase discontinuously (suddenly) by the same amount at a shared event; they change in parallel\n",
    "# over time, but they have different starting values.  Create a model that combines group fixed effects with an event study, as suggested in the online reading.\n",
    "# Explain what you did, how the model works, and how it accounts for both baseline differences and the common event effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0745      0.071      1.055      0.294      -0.065       0.214\n",
      "group[T.B]     1.7941      0.067     26.861      0.000       1.662       1.926\n",
      "group[T.C]    -0.9561      0.067    -14.314      0.000      -1.088      -0.824\n",
      "t              0.0841      0.005     17.786      0.000       0.075       0.093\n",
      "post           1.3783      0.109     12.625      0.000       1.162       1.595\n",
      "==============================================================================\n",
      "\n",
      "Mean change (post - pre) by group:\n",
      "group\n",
      "A    2.945925\n",
      "B    3.112874\n",
      "C    3.122626\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9417/3065229535.py:32: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print(df.groupby('group').apply(lambda d: d.loc[d.post==1,'y'].mean() - d.loc[d.post==0,'y'].mean()))\n",
      "/tmp/ipykernel_9417/3065229535.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  print(df.groupby('group').apply(lambda d: d.loc[d.post==1,'y'].mean() - d.loc[d.post==0,'y'].mean()))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "G = ['A','B','C']\n",
    "T = 40\n",
    "t0 = 20                     \n",
    "jump = 1.5                  \n",
    "slope = 0.08                \n",
    "baseline = {'A':0.0, 'B':2.0, 'C':-1.0}\n",
    "\n",
    "rows = []\n",
    "for g in G:\n",
    "    for t in range(T):\n",
    "        post = 1 if t >= t0 else 0\n",
    "        y = baseline[g] + slope*t + jump*post + np.random.normal(0, 0.3)\n",
    "        rows.append((g, t, post, y))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=['group','t','post','y'])\n",
    "df['group'] = df['group'].astype('category')  \n",
    "\n",
    "\n",
    "model = smf.ols('y ~ group + t + post', data=df).fit()\n",
    "print(model.summary().tables[1])\n",
    "\n",
    "\n",
    "print(\"\\nMean change (post - pre) by group:\")\n",
    "print(df.groupby('group').apply(lambda d: d.loc[d.post==1,'y'].mean() - d.loc[d.post==0,'y'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40699d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made three groups: A, B, C \n",
    "# Each group starts at a different base line level (height)\n",
    "# In addition, random noise was added \n",
    "# Formula y ~ group + t + post\n",
    "# Group - each group, starts at its own baseline level \n",
    "# t = time - to track the sready rise over time \n",
    "# post = measure the jump after time \n",
    "# group part removes the differences at the starting point \n",
    "# event part find the average size of the shared jump "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81755575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 12 Reflection \n",
    "# Construct a dataset in which prior trends do not hold, and in which this makes the differences-in-differences come out wrong.  Explain why the\n",
    "# differences-in-differences estimate of the effect comes out higher or lower than the actual effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e33b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     group period  mean_y\n",
      "0  control    pre     5.0\n",
      "1  control   post     5.0\n",
      "2  treated    pre     6.0\n",
      "3  treated   post     9.0\n",
      "\n",
      "Control change: 0.0\n",
      "Treated change: 3.0\n",
      "DiD estimate  : 3.0\n",
      "True effect   : 2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "pre_trend_treated = 1.0   \n",
    "pre_trend_control = 0.0   \n",
    "true_effect = 2.0         \n",
    "\n",
    "\n",
    "mean_ctl_pre = 5\n",
    "mean_ctl_post = mean_ctl_pre + pre_trend_control  # flat\n",
    "mean_trt_pre = 6\n",
    "mean_trt_post = mean_trt_pre + pre_trend_treated + true_effect\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"group\": [\"control\",\"control\",\"treated\",\"treated\"],\n",
    "    \"period\": [\"pre\",\"post\",\"pre\",\"post\"],\n",
    "    \"mean_y\": [mean_ctl_pre, mean_ctl_post, mean_trt_pre, mean_trt_post]\n",
    "})\n",
    "\n",
    "\n",
    "change_ctl = mean_ctl_post - mean_ctl_pre\n",
    "change_trt = mean_trt_post - mean_trt_pre\n",
    "did = change_trt - change_ctl\n",
    "\n",
    "print(df)\n",
    "print(\"\\nControl change:\", change_ctl)\n",
    "print(\"Treated change:\", change_trt)\n",
    "print(\"DiD estimate  :\", did)\n",
    "print(\"True effect   :\", true_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Effect = 2 - the actual event made the treatment group go up by 2 points \n",
    "# DiD estimate is 3 \n",
    "# Before the event the treated group was going up but control group did not experience a change \n",
    "# DiD thought they would both move even without an event that is why it is estimated 3 but in reality that was not the case - DiD overestimated "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
